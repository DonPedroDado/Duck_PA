{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic web scraping bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This bot is used to find pdfs of linear equations for training of an OCR used to read formulas. In the future other topics will be added(quadratic equations, integrals, derivatives, and more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download all the necessary libraries. For this bot we will combine beautifulsoup and selenium. We must do this because google scholar detects automatic web scraping and blocks the bots. So we can't only use beautifulsoup. Beautilsoup only scrapes the HTML and ignores the javascript, so first we will use selenium to read the javascript and then beautifulsoup to scrape the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Provides functions for interacting with the operating system (e.g., file and folder manipulation).\n",
    "import re  # Provides support for regular expressions to handle pattern matching, like searching for \".pdf\".\n",
    "import time  # Used for adding time delays, making the script wait for a specified period.\n",
    "import random  # Generates random numbers, which is used to create random sleep times to mimic human behavior.\n",
    "import requests  # Allows the script to send HTTP requests to retrieve content from websites.\n",
    "from selenium import webdriver  # Provides tools for controlling web browsers programmatically.\n",
    "from selenium.webdriver.chrome.service import Service  # Allows setting up the Chrome WebDriver service.\n",
    "from selenium.webdriver.chrome.options import Options  # Enables configuration of Chrome's options, like running headless.\n",
    "from selenium.webdriver.common.by import By  # Provides methods for locating elements on a webpage (e.g., by class name, XPath).\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # Automatically manages the installation of the appropriate ChromeDriver version.\n",
    "from bs4 import BeautifulSoup  # A library for parsing HTML and XML documents, used to extract information from web pages.\n",
    "from urllib.parse import urljoin  # A function to handle relative URLs, ensuring that links are converted to absolute URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the folder for saving the pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to save PDF files\n",
    "SAVE_FOLDER = \"/Users/donpedrodado/Documents/opt/Duck_PA/math_training_sets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets up Selenium WebDriver with headless Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    # Initialize the Chrome options object\n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Run without opening the browser window (headless mode)\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    \n",
    "    # Prevent detection of Selenium as an automated tool\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")  \n",
    "    \n",
    "    # Additional options for better performance in a headless environment\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Initialize the Chrome driver with the specified options\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads a PDF file from a given URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url):\n",
    "    \"\"\"Downloads a PDF from the given URL.\"\"\"\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL with a 10-second timeout\n",
    "        response = requests.get(url, stream=True, timeout=10)\n",
    "        \n",
    "        # Raise an exception if the response status code indicates an error (non-2xx)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract the filename from the URL (last part after the last \"/\")\n",
    "        filename = url.split(\"/\")[-1]\n",
    "\n",
    "        # Create the full file path where the PDF will be saved\n",
    "        filepath = os.path.join(SAVE_FOLDER, filename)\n",
    "\n",
    "        # Open the file in write-binary mode and download the PDF in chunks\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):  # Download in chunks of 8 KB\n",
    "                file.write(chunk)\n",
    "\n",
    "        # Print a success message with the filename\n",
    "        print(f\"✅ Downloaded: {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # If an error occurs (like connection failure or invalid URL), print an error message\n",
    "        print(f\"❌ Failed to download {url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract PDF links from an external page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdfs_from_page(url):\n",
    "    try:\n",
    "        # Make an HTTP GET request to the provided URL\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Parse the page content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Initialize an empty list to hold the PDF links\n",
    "        pdf_links = []\n",
    "\n",
    "        # Loop through all anchor tags (<a>) that have an href attribute (i.e., links)\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]  # Get the href attribute (the URL)\n",
    "\n",
    "            # Handle relative URLs by joining them with the base URL\n",
    "            full_url = urljoin(url, href)  # This resolves relative URLs to absolute ones\n",
    "\n",
    "            # If the link ends with '.pdf', it's likely a direct link to a PDF file\n",
    "            if full_url.endswith(\".pdf\"):  # Check if it's a PDF link\n",
    "                pdf_links.append(full_url)  # Add the PDF link to the list\n",
    "\n",
    "            #TODO: fix parsing of HTML with beatifulsoup\n",
    "        # Return the list of PDF links found on this page\n",
    "        return pdf_links\n",
    "    except Exception as e:\n",
    "        # If an error occurs (e.g., network issues), print an error message\n",
    "        print(f\"❌ Error extracting PDFs from {url}: {e}\")\n",
    "        return []  # Return an empty list if an error occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Google Scholar and extract PDFs from external sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdfs(num_results=5):\n",
    "    \"\"\"Search Google Scholar and extract PDFs from external sites.\"\"\"\n",
    "    \n",
    "    # Set up the Selenium WebDriver to interact with the Google Scholar page\n",
    "    driver = setup_driver()\n",
    "\n",
    "    # Set the search URL for Google Scholar with a search term for \"linear equations\"\n",
    "    search_url = f\"https://scholar.google.com/scholar?q=linear+equations\"\n",
    "    driver.get(search_url)  # Open the search results page in the browser\n",
    "    \n",
    "    time.sleep(random.uniform(3, 6))  # Sleep for a random amount of time (3-6 seconds) to mimic human behavior\n",
    "\n",
    "    # Initialize a list to hold external links (to PDFs or articles)\n",
    "    external_links = []\n",
    "    \n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    # Iterate through all the anchor tags (<a>) with href attributes\n",
    "    for result in soup.find_all(\"a\", href=True):\n",
    "        link = result[\"href\"]  # Get the href attribute (URL) of the link\n",
    "        \n",
    "        # Handle relative URLs by ensuring they are absolute URLs\n",
    "        if link.startswith(\"//\"):\n",
    "            link = \"https:\" + link  # Prepend https: if the URL starts with //\n",
    "\n",
    "        # Check if the link contains \"[PDF]\" in the text or ends with \".pdf\"\n",
    "        if \"[PDF]\" in result.text or re.search(r\".*\\.pdf$\", link):\n",
    "            external_links.append(link)  # If it’s a PDF link, add it to the external_links list\n",
    "\n",
    "    # If no direct PDF links are found, look for article links instead\n",
    "    if not external_links:\n",
    "        for link in soup.find_all(\"a\", href=True):  # Iterate over all the links again\n",
    "            external_links.append(link[\"href\"])  # Collect all other external article links\n",
    "\n",
    "    driver.quit()  # Close the browser driver\n",
    "\n",
    "    # Now, visit those external links and extract any PDFs they might contain\n",
    "    pdf_links = []  # Initialize a list to store the final PDF links\n",
    "    for ext_link in external_links[:num_results]:  # Loop through the first `num_results` external links\n",
    "        pdf_links.extend(extract_pdfs_from_page(ext_link))  # Extract PDFs from the external page\n",
    "\n",
    "    return pdf_links[:num_results]  # Return the first `num_results` PDF links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to execute the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Check if the save folder exists, and create it if it doesn't\n",
    "    if not os.path.exists(SAVE_FOLDER):\n",
    "        os.makedirs(SAVE_FOLDER)\n",
    "\n",
    "    # Call find_pdfs() to search for PDF links (returns a list of PDFs)\n",
    "    pdf_links = find_pdfs()\n",
    "\n",
    "    # If PDF links were found, download each PDF\n",
    "    if pdf_links:\n",
    "        for pdf in pdf_links:\n",
    "            download_pdf(pdf)\n",
    "    else:\n",
    "        # If no PDF links were found, print a message\n",
    "        print(\"❌ No PDF links found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error extracting PDFs from https://annals.math.princeton.edu/wp-content/uploads/annals-v171-n3-p08-p.pdf: The markup you provided was rejected by the parser. Trying a different parser or a different encoding may help.\n",
      "\n",
      "Original exception(s) from parser:\n",
      " AssertionError: expected name token at \"<![�$�iu�!\\x08R'�#z���n\"\n",
      "❌ No PDF links found.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # If the script is being run directly (not imported), call the main function\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
